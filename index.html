<!DOCTYPE html>
<html lang="en" xmlns="http://www.w3.org/1999/html">
<head>
    <meta charset="UTF-8">
    <meta name="viewport"
          content="width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
    <meta http-equiv="Expires" content="0">
    <meta http-equiv="Cache-Control" content="no-cache">
    <meta http-equiv="Pragma" content="no-cache">
    <meta http-equiv="Cache" content="no-cache">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <!--    icon-->
    <link rel="shortcut icon" href="./favicon.ico" type="image/x-icon" />
    <!--    css files-->
    <link href="css/init.css" type="text/css" rel="stylesheet">
    <link href="css/style.css" type="text/css" rel="stylesheet">
    <!--    title-->
    <title>Deep Hierarchical Learning for 3D Segmentation</title>
    <!--    description-->
    <meta name="Description"
          content="Official website of the research project 'Deep Hierarchical Learning for 3D Segmentation'.">
    <!--    keywords-->
    <meta name="Keywords"
          content="Chongshou Li, Yuheng Liu, Xinke Li, Yuning Zhang, Tianrui Li, Junsong Yuan">
    <script src="https://kit.fontawesome.com/766b4d68b2.js" crossorigin="anonymous"></script>
</head>

<body>

<div class="paper-info">
    <div class="paper-title-new" style="font-size: 38px">
        Deep Hierarchical Learning for 3D Segmentation
    </div>
<!--    <div class="paper-authors-new">-->
<!--        <div><a href="https://scholar.google.com.sg/citations?user=pQsr70EAAAAJ&hl=en"><b><u>Chongshou Li</u></b></a><sup>1</sup><b>,&nbsp;</b></div>-->
<!--        <div><a href="https://yuheng.ink"><b><u>Yuheng Liu</u></b></a><sup>1,2</sup><b>,&nbsp;</b></div>-->
<!--        <div><a href="https://shinke-li.github.io/"><b><u>Xinke Li</u></b></a><sup>3</sup><b>,&nbsp;</b></div>-->
<!--        <div><a href="" class="coming-soon"><b><u>Yuning Zhang</u></b></a><sup>1,2</sup><b>,&nbsp;</b></div>-->
<!--        <div><a href="https://scholar.google.com/citations?hl=en&authuser=1&user=CQ1HneMAAAAJ"><b><u>Tianrui Li</u></b></a><sup>1</sup><b>,&nbsp;</b></div>-->
<!--        <div><a href="https://cse.buffalo.edu/~jsyuan/"><b><u>Junsong Yuan</u></b></a><sup>4</sup></div>-->
<!--    </div>-->
<!--    <div class="paper-institutions" align="center">-->
<!--        <div class="paper-sub-institute">-->
<!--            <div><sup>1</sup>Southwest Jiaotong University,&nbsp;-->
<!--            </div><div><sup>2</sup>University of Leeds,&nbsp;</div>-->
<!--            <div><sup>3</sup>National University of Singapore,&nbsp;</div>-->
<!--            <div><sup>4</sup>University at Buffalo</div>-->
<!--        </div>-->
<!--        <div class="paper-sub-institute">-->

<!--        </div>-->
<!--    </div>-->

    <div class="paper-link-group">
        <a href="" class="paper-sub-link" target="_blank" style="width: 100px">
            <img src="icons/arxiv-logomark-small.svg" style="width: 16px; height: 16px">&nbsp;&nbsp;Arxiv
        </a>
        <a href="" class="paper-sub-link" target="_blank" style="width: 100px">
            <i class="fas fa-file-pdf"></i>&nbsp;&nbsp;Paper
        </a>
        <a href="https://github.com/dhl3d/deep-hierarchical-learning-for-3d-segmentation" class="paper-sub-link" target="_blank" style="width: 100px">
            <i class="fa-brands fa-github"></i>&nbsp;&nbsp;Code
        </a>
    </div>

    <div class="paper-category">
        <div class="circle-logo">
            <img src="images/dhl3d_logo.png" style="width: 70%; height: 70%">
        </div>
        <p class="category-name">Semantic Segmentation</p>
    </div>
</div>

<div class="border-box">
    <div style="height: 1px;" data-width="100%" class="divider-border"></div>
</div>

<div class="paper-sub-section">
    <div class="title">
        ABSTRACT
    </div>
<!--    <img src="images/teaser.png" style="width: 40%" class="paper-img-phone">-->
    <div class="paper-contents">
        <p>
            In the realm of computer vision, 3D objects and
            scenes are inherently hierarchical, offering an opportunity to
            break down complex visual entities into simpler components
            and abstract the visual world across multiple levels. However,
            effectively harnessing this hierarchy remains a critical challenge
            in the field of 3D segmentation. One of the key issues is
            ensuring that predictions at different hierarchical levels maintain
            coherence. In this paper, we define hierarchical learning within
            a probabilistic framework and introduce an aggregation matrix
            to quantitatively model the relationships among different levels
            of the hierarchy. Building upon these foundations, we design
            coherence loss functions and establish a theoretical relationship
            between class accuracy and coherence. To facilitate effective
            learning of the hierarchy, we present a deep hierarchical learning
            architecture that includes a dedicated hierarchical embedding
            learning module. Additionally, we propose a method that leverages a pre-trained Large Language Model and clustering techniques to generate a hierarchical structure for fine-grained 3D
            segmentation. Our extensive experiments demonstrate the effectiveness of the proposed solution in addressing these challenges.
        </p>
    </div>
</div>

<div class="border-box">
    <div style="height: 1px;" data-width="100%" class="divider-border"></div>
</div>

<div class="paper-sub-section">
    <div class="title" style="padding-bottom: 40px">
        METHOD
    </div>
    <div class="paper-sub-title">
        Architecture of Hierarchical Learning
    </div>
    <img src="images/method_dhl3d.png" style="width: 50%" class="paper-img-phone">
    <div class="paper-contents">
        <p>
            This architecture takes the raw point cloud
            as input and facilitates the learning of hierarchically coherent
            predictions. It consists of three key components:
            <ul style="list-style-type: disc!important; margin-top: 10px">
                <li>
                    <b>Encoder and Multi-Decoders (EMD):</b> The EMD is a
                    multi-task learning network that incorporates a shared
                    encoder and multiple parallel decoders, with each decoder
                    corresponding to a hierarchical layer. In Section V, we explore the use of three popular 3D segmentation backbones
                    as encoders, including PointNet++, RandLANet,
                    and SparseUNet. The EMD focuses on encoding geometric information while disregarding the rich semantic
                    relations among classes at different hierarchical layers
                    simultaneously.
                </li>
                <br>
                <li>
                    <b>Hierarchical Embedding Fusion Module (HEFM):</b>
                    The HEFM leverages structured knowledge (i.e., TDCC
                    and BDCC) to reshape point embeddings, generating
                    hierarchy-aware point representations and enhancing coherence in predictions
                </li>
                <br>
                <li>
                    <b>Hierarchical Coherence (HC) Loss:</b> Our proposed network is trained end-to-end by minimizing a combination
                    of the HC loss and cross-entropy loss.
                </li>
            </ul>
        </p>
    </div>

    <div class="paper-sub-title" style="margin-top: 40px">
        Hierarchical Embedding Fusion Module
    </div>
    <img src="images/hefm.png" style="width: 20%" class="paper-img-phone">
    <div class="paper-contents">
        <p>
            The TDCC and BDCC constraints outline the necessary conditions for hierarchically coherent predictions.
            We contemplate employing them BDCC as
            constraints for HL. However, direct utilization of TDCC and
            BDCC is challenging and redundant for the loss in Section 4.3.
            We consider adapting them into soft constraints for multiple
            samples, i.e., inter-points constraints. The specific expression
            of this adaptation is that if two points have inconsistent
            predictions at the super-class layer, their predictions at the
            sub-class layer must be inconsistent as well; if their predictions
            at the sub-class layer are consistent, then their predictions at
            the super-class layer must also be consistent. These constraints
            are integrated into the Hierarchical Embedding Fusion Module
            (HEFM). The HEFM comprises two essential components:
        <ul style="list-style-type: disc !important; margin-top: 10px">
            <li>
                <b>Top-aware Fusion:</b> It ensures that points associated with
                the same super-class labels in the top-level should be
                proximate to each other within the embedding space of
                the bottom-level.
            </li>
            <br>
            <li>
                <b>Bottom-guided Aggregation:</b> It enforces that points
                linked to the same subclass labels in the bottom-level
                should exhibit similarity within the top-level embedding
                space.
            </li>
        </ul>
        </p>
    </div>

    <div class="paper-sub-title" style="margin-top: 40px">
        Hierarchical Embedding Fusion Module
    </div>
    <img src="images/lhm.png" style="width: 20%" class="paper-img-phone">
    <div class="paper-contents">
        <p>
            The preceding sections detailed a method for training segmentation models on hierarchically annotated 3D datasets. Yet,
            the significant annotation effort required for such datasets often
            limits their availability. To broaden our approachâ€™s
            applicability, we introduce a technique to construct a class
            hierarchy from the fine-grained class labels within the dataset.
            Our methodology exploits a Vision Language Model (VLM)
            to derive class embeddings. Subsequently, these embeddings
            inform a hierarchical clustering process, producing a dendrogram of classes. Itâ€™s essential to underscore that constructing
            a label taxonomy for hierarchical 3D segmentation should
            account for both the semantic relevance and the geometric
            characteristics of the class. Recent advances in VLMs, particularly in aligning geometric features in images with language
            embeddings, enable us to harness pre-trained embeddings for
            this hierarchy extraction. We utilized the widely-acknowledged
            CLIP text encoder, trained on the WebImage Text Dataset,
            to encode the fine-grained classes present in the 3D dataset.
            For improved precision, we employed the datasetâ€™s class
            definitions as caption text, rather than solely the class terms,
            thus mitigating potential word embedding ambiguities. These
            embeddings then serve as features, guiding the iterative merging of clusters from the original fine-grained classes into a
            class dendrogram. Ultimately, we employ a large language
            model (LLM) to prune the generated dendrogram, resulting
            in a semantically coherent class hierarchy. This entire process
            is depicted in Fig
        </p>
    </div>


</div>

<div class="border-box">
    <div style="height: 1px;" data-width="100%" class="divider-border"></div>
</div>

<div class="paper-sub-section">
    <div class="title" style="padding-bottom: 40px">
        RESULTS
    </div>
    <img src="images/fig1.png" style="width: 40%" class="paper-img-phone">
    <div class="paper-sub-title">
        Point Cloud Semantic Segmentation on Campus3D
    </div>
    <img src="images/table1.png" style="width: 40%" class="paper-img-phone">
    <div class="paper-contents">
        <p>
            Across nearly all hierarchical levels and models, we observe
            consistent performance gains when employing HL. The sole
            deviation from this trend is with the multi-classifier variant
            of PointNet++ (i.e., PointNet++ without HL), which outperforms its HL counterpart at the most coarse-grained level.
            A potential explanation for this could be that the inherent
            structure and feature representation of the raw PointNet++
            model is better suited for coarse-grained segmentation tasks,
            whereas HL might introduce complexities that marginally
            diminish performance at that specific level. Nevertheless, the
            overall superior performance of the HL mechanism over
            the traditional multi-classifier underscores its potency in 3D
            hierarchical semantic segmentation. A potential explanation
            for the HL methodâ€™s enhanced performance is that the intrinsic
            relationships among hierarchical label layers may provide
            supplementary geometric information beneficial for semantic
            segmentation
        </p>
    </div>
</div>


<!--<div class="border-box">-->
<!--    <div style="height: 1px;" data-width="100%" class="divider-border"></div>-->
<!--</div>-->
<!--<div class="paper-sub-section">-->
<!--    <div class="title" style="padding-bottom: 40px">-->
<!--        BibTex-->
<!--    </div>-->
<!--    <div class="paper-bibtex">-->
<!--        <code>-->
<!--        </code>-->
<!--    </div>-->
<!--</div>-->

<div class="paper-footer">
    <p>This website was designed and built by our team.</p>
    <div class="recording">
        <a href="https://clustrmaps.com/site/1bxkd"  title="Visit tracker" style="visibility: hidden"><img src="//www.clustrmaps.com/map_v2.png?d=0mhtt3HacryNm3uQ4dzefqYes4XnwM4-WHqqI_TkAuA&cl=ffffff" style="display: none"/></a>
    </div>
</div>
</body>
</html>